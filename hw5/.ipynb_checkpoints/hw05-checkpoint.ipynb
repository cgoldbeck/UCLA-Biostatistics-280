{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat M280 Homework 5\n",
    "\n",
    "Cameron S. Goldbeck\n",
    "\n",
    "**Due June 16 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider again the MLE of the Dirichlet-multinomial model. In [HW4](http://hua-zhou.github.io/teaching/biostatm280-2017spring/hw/hw04.html), we worked out a Newton's method. In this homework, we explore the EM and MM approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1\n",
    "\n",
    "Show that, given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, the log-likelihood can be written as\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k).\n",
    "$$\n",
    "Hint: $\\Gamma(a + k) / \\Gamma(a) = a (a + 1) \\cdots (a+k-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) From HW 4 we were given \n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\Gamma(\\alpha_j + x_{ij}) - \\ln \\Gamma(\\alpha_j)] - \\sum_{i=1}^n [\\ln \\Gamma(|\\alpha|+|\\mathbf{x}_i|) - \\ln \\Gamma(|\\alpha|)]=$$\n",
    "\n",
    "$$\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln \\frac{\\Gamma(\\alpha_j + x_{ij})}{\\Gamma(\\alpha_j)}] - \\sum_{i=1}^n [\\ln \\frac{\\Gamma(|\\alpha|+|\\mathbf{x}_i|)}{\\Gamma(|\\alpha|)}]\n",
    "$$\n",
    "\n",
    "Given the hint, we have that the above is equal to \n",
    "\n",
    "\n",
    "$$L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} + \\sum_{i=1}^n \\sum_{j=1}^d [\\ln (\\alpha_j(\\alpha_j+1)\\dots(\\alpha_j+x_{ij}-1)] - \\sum_{i=1}^n [\\ln |\\alpha|(|\\alpha|+1)\\dots(|\\alpha|+|\\mathbf{x}_i|-1)]=\n",
    "$$\n",
    "\n",
    "$$\\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2\n",
    "\n",
    "Suppose $(P_1,\\ldots,P_d) \\in \\Delta_d = \\{\\mathbf{p}: p_i \\ge 0, \\sum_i p_i = 1\\}$ follows a Dirichlet distribution with parameter $\\alpha = (\\alpha_1,\\ldots,\\alpha_d)$. Show that\n",
    "$$\n",
    "\t\\mathbf{E}(\\ln P_j) = \\Psi(\\alpha_j) - \\Psi(|\\alpha|),\n",
    "$$\n",
    "where $\\Psi(z) = \\Gamma'(z)/\\Gamma(z)$ is the digamma function and $|\\alpha| = \\sum_{j=1}^d \\alpha_j$. Hint: Differentiate the identity \n",
    "$$\n",
    "1 = \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)}{\\prod_{j=1}^d \\Gamma(\\alpha_j)} \\prod_{j=1}^d p_j^{\\alpha_j-1} \\, d\\mathbf{p}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) We begin by differentiating each side of the above identity with respect to $\\alpha_j$. We note first we apply the product rule between the fraction and the product, and then the quotient rule on the fraction.\n",
    "\n",
    "$$0 = \\int_{\\Delta_d} \\frac{\\Gamma'(|\\alpha|)\\prod_{i=1}^d \\Gamma(\\alpha_i)-\\Gamma(|\\alpha|)\\Gamma'(\\alpha_j) \\prod_{i\\ne j}^d \\Gamma(\\alpha_i)}{(\\prod_{i=1}^d \\Gamma(\\alpha_i))^2}\\prod_{i=1}^d p_i^{\\alpha_i-1} + \\frac{\\Gamma(|\\alpha|)}{\\prod_{i=1}^d \\Gamma(\\alpha_i)} \\ln(p_j)\\prod_{i=1}^d p_i^{\\alpha_i-1} \\, d\\mathbf{p}$$\n",
    "\n",
    "We can rearrange the above to give\n",
    "\n",
    "$$ \\int_{\\Delta_d} \\frac{\\Gamma(|\\alpha|)\\Gamma'(\\alpha_j) \\prod_{i\\ne j}^d \\Gamma(\\alpha_i)-\\Gamma'(|\\alpha|)\\prod_{i=1}^d \\Gamma(\\alpha_i)}{(\\prod_{j=1}^d \\Gamma(\\alpha_j))^2}\\prod_{i=1}^d p_i^{\\alpha_i-1}\\, d\\mathbf{p}=\n",
    "\\int_{\\Delta_d}\\frac{\\Gamma(|\\alpha|)}{\\prod_{i=1}^d \\Gamma(\\alpha_i)} \\ln(p_j)\\prod_{i=1}^d p_i^{\\alpha_i-1} \\, d\\mathbf{p}=\\mathbf{E}(\\ln(P_j))$$\n",
    "\n",
    "This yields\n",
    "\n",
    "$$\\frac{\\Gamma(|\\alpha|)\\Gamma'(\\alpha_j) \\prod_{i\\ne j}^d \\Gamma(\\alpha_i)-\\Gamma'(|\\alpha|)\\prod_{i=1}^d \\Gamma(\\alpha_i)}{(\\prod_{i=1}^d \\Gamma(\\alpha_i))^2} \\cdot \\frac{\\prod_{i=1}^d \\Gamma(\\alpha_i)}{\\Gamma(|\\alpha|)}=\\mathbf{E}(\\ln(P_j))$$\n",
    "\n",
    "$$\\implies \\frac{\\Gamma'(\\alpha_j)}{\\Gamma(\\alpha_j)}-\\frac{\\Gamma'(|\\alpha|)}{\\Gamma(|\\alpha|)}=\\mathbf{E}(\\ln(P_j))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3\n",
    "\n",
    "The admixture representation of the Dirichlet-multinomial distribution suggests that we can treat the unobserved multinomial parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ as missing data and derive an EM algorithm. Show that the Q function is\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$\n",
    "where $c^{(t)}$ is a constant irrelevant to optimization. Comment on why it is not easy to maximize the Q function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Introducing unobserved parameters $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ our new likelihood $\\prod _{i=1}^nf(\\mathbf{p}_i,\\mathbf{x}_i, \\mathbf{\\alpha}_i)$. We then have \n",
    "\n",
    "$$Q(\\alpha|\\alpha^{(t)}) = \\mathbf{E}(\\prod _{i=1}^nf(\\mathbf{p}_i,\\mathbf{x}_i, \\mathbf{\\alpha}_i))$$\n",
    "\n",
    "We note that we have seen $\\mathbf{p}_1,\\ldots,\\mathbf{p}_n$ follow a Dirichlet multinomial with parameters $\\mathbf{x}_1+\\alpha_1,\\ldots,\\mathbf{x}_n+\\alpha_n$. We then have for a single $p_{ij}$\n",
    "\n",
    "$$\\mathbf{E}(f(p_{ij},\\mathbf{x}_i, \\mathbf{\\alpha}_i)) =  \\mathbf{E}(f(\\mathbf{p}_i,\\mathbf{x}_i, \\mathbf{\\alpha}_i)) =   \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i} +  \\sum_{j=1}^d [x_{ij} +\\alpha_j^{(t)} -1]E(\\ln(p_{ij})) -  [ \\ln \\Gamma(\\alpha_j) - \\ln \\Gamma(|\\alpha|)]$$ \n",
    "\n",
    "Now we can apply what we derived in Q2, and we collect constants that do not involve $\\alpha_j$.\n",
    "\n",
    "$$=\\sum_{j=1}^d \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c$$\n",
    "\n",
    "If we sum over all $i=1,\\dots,n$, we get $Q(\\alpha|\\alpha^{(t)})$\n",
    "\n",
    "$$\n",
    "    Q(\\alpha|\\alpha^{(t)}) = \n",
    "\\sum_{j=1}^d \\sum_{i=1}^n \\alpha_j \\left[\\Psi(x_{ij}+\\alpha_j^{(t)}) - \\Psi(|\\mathbf{x}_i|+|\\alpha^{(t)}|)\\right] - n \\sum_{j=1}^d \\ln \\Gamma(\\alpha_j) + n \\ln \\Gamma(|\\alpha|) + c^{(t)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4\n",
    "\n",
    "We derive an MM algorithm for maximing $L$. Consider the formulation of the log-likelihood that contains terms $\\ln (\\alpha_j + k)$ and $- \\ln (|\\alpha|+k)$. Applying Jensen's inequality to the concave term $\\ln (\\alpha_j + k)$ and supporting hyperplane inequality to the convex term $- \\ln (|\\alpha|+k)$, show that a minorizing function to $L(\\alpha)$ is\n",
    "$$\n",
    "\tg(\\alpha|\\alpha^{(t)}) = - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} |\\alpha| + \\sum_{j=1}^d \\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k} \\ln \\alpha_j + c^{(t)},\n",
    "$$\n",
    "where $s_{jk} = \\sum_{i=1}^n 1_{\\{x_{ij} > k\\}}$, $r_k = \\sum_{i=1}^n 1_{\\{|\\mathbf{x}_i| > k\\}}$, and  $c^{(t)}$ is a constant irrelevant to optimization. Maximizing the surrogate function $g(\\alpha|\\alpha^{(t)})$ is trivial since $\\alpha_j$ are separated. Show that the MM updates are\n",
    "$$\n",
    "\t\\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}} \\alpha_j^{(t)}, \\quad j=1,\\ldots,d.\n",
    "$$\n",
    "The quantities $s_{jk}$, $r_k$, $\\max_i x_{ij}$ and $\\max_i |\\mathbf{x}_i|$ only depend on data and can be pre-computed. Comment on whether the MM updates respect the parameter constraint $\\alpha_j>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) First, in order to get the minorizing fuction we have\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^n \\ln \\binom{|\\mathbf{x}_i|}{\\mathbf{x}_i}\n",
    "+\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) - \\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)=$$\n",
    "\n",
    "$$\\sum_{i=1}^n \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) 1_{[x_{ij}>k]}-\n",
    "\\sum_{i=1}^n \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)1_{[|x_i|>k]} + c^{(t)}=$$\n",
    "\n",
    "Now we define the following\n",
    "\n",
    "$$s_{jk}=\\sum_{i=1}^n 1_{x_{ij}>k} \\qquad r_k=\\sum_{i=1}^n 1_{|x_i|>k}$$\n",
    "\n",
    "This gives \n",
    "$$\n",
    "L(\\alpha) =  \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) s_{jk}- \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)r_k + c^{(t)}\n",
    "$$\n",
    "\n",
    "Now we can apply Jensen's inequality.\n",
    "\n",
    "$$\\ln(\\alpha_j+k) \\ge \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln\\bigg(\\frac{\\alpha_j^{(t)}+k}{\\alpha_j^{(t)}}\\cdot \\alpha_j\\bigg)+\\frac{k}{\\alpha_j^{(t)}+k}\\ln\\bigg(\\frac{\\alpha_j^{(t)}+k}{k}\\cdot k\\bigg)=\\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j)+c^{(t)}$$\n",
    "\n",
    "Then, we use the hyperplane inequality.\n",
    "\n",
    "$$\\ln(|\\alpha|+k)\\le \\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}+\\ln(|\\alpha|+k)=\n",
    "\\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}$$\n",
    "\n",
    "Finally, we can give the minorizing fuction\n",
    "\n",
    "$$\n",
    "L(\\alpha) =  \\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\ln(\\alpha_j+k) s_{jk}- \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\ln(|\\alpha|+k)r_k + c^{(t)}\\ge \n",
    "\\sum_{j=1}^d \\sum_{k=0}^{x_{ij}-1} \\frac{\\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}\\ln(\\alpha_j) s_{jk}- \\sum_{k=0}^{|\\mathbf{x}_i|-1} \\frac{|\\alpha|-|\\alpha^{(t)}|}{|\\alpha^{(t)}|+k}r_k + c^{(t)}=g(\\alpha|\\alpha^{(t)})\n",
    "$$\n",
    "\n",
    "\n",
    "Once we have the minorization function, we can take the partial derivative with respect to $\\alpha_j$. \n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\alpha_j}g(\\alpha|\\alpha(t))= - \\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k} +\\frac{1}{\\alpha_j}\\cdot\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}$$\n",
    "\n",
    "Now we solve the above for zero.\n",
    "\n",
    "$$\\alpha_j = \\alpha_j^{(t+1)} = \\frac{\\sum_{k=0}^{\\max_i x_{ij}-1} \\frac{s_{jk} \\alpha_j^{(t)}}{\\alpha_j^{(t)}+k}}{\\sum_{k=0}^{\\max_i |\\mathbf{x}_i|-1} \\frac{r_k}{|\\alpha^{(t)}|+k}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5\n",
    "\n",
    "Write a function for finding MLE of Dirichlet-multinomial distribution given iid observations $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$, using MM algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_logpdf(AbstractArray{T<:Any, 1}, Any) in module Main at In[6]:3 overwritten at In[9]:3.\n",
      "WARNING: Method definition dirmult_logpdf(AbstractArray{T<:Any, 2}, Any) in module Main at In[6]:30 overwritten at In[9]:30.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'dirmult_logpdf :: Tuple{AbstractArray{T,2},Any}' in module 'Main'.\u001b[0m\n",
      "WARNING: Method definition dirmult_score(AbstractArray{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[6]:38 overwritten at In[9]:38.\n",
      "WARNING: Method definition dirmult_score!(Array{T<:Any, 1}, AbstractArray{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[6]:44 overwritten at In[9]:44.\n",
      "WARNING: Method definition dirmult_score!(Array{T<:Any, 1}, AbstractArray{T<:Any, 1}, Array{T<:Any, 1}) in module Main at In[6]:57 overwritten at In[9]:57.\n",
      "WARNING: Method definition dirmult_obsinfo(AbstractArray{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[6]:79 overwritten at In[9]:79.\n",
      "WARNING: Method definition dirmult_obsinfo!(Array{T<:Any, 1}, AbstractArray{T<:Any, 2}, Array{T<:Any, 1}) in module Main at In[6]:85 overwritten at In[9]:85.\n",
      "WARNING: Method definition dirmult_obsinfo!(Array{T<:Any, 1}, AbstractArray{T<:Any, 1}, Array{T<:Any, 1}) in module Main at In[6]:95 overwritten at In[9]:95.\n",
      "WARNING: Method definition dirmult_mom(Array{#T<:Real, 2}) in module Main at In[6]:118 overwritten at In[9]:118.\n",
      "WARNING: Method definition dirmult_newton(AbstractArray{T<:Any, 2}) in module Main at In[6]:161 overwritten at In[9]:161.\n",
      "WARNING: Method definition #dirmult_newton(Array{Any, 1}, Main.#dirmult_newton, AbstractArray{T<:Any, 2}) in module Main overwritten.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_newton (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here is Dr. Zhou's code from HW 4 for logpfd, score, hess, mom, and Newton's method functions\n",
    "function dirmult_logpdf(x::AbstractVector, α)\n",
    "    T = promote_type(eltype(x), eltype(α))\n",
    "    xs = sum(x)\n",
    "    αs = sum(α)\n",
    "    if xs == 0 && αs == 0\n",
    "        return zero(T)\n",
    "    elseif xs > 0 && αs == 0\n",
    "        return convert(T, -Inf)\n",
    "    else\n",
    "        l = lfact(xs) - lgamma(xs + αs) + lgamma(αs)\n",
    "    end\n",
    "    for i in eachindex(x)\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            return convert(T, -Inf)\n",
    "        elseif α[i] > 0\n",
    "            l += - lfact(x[i]) + lgamma(x[i] + α[i]) - lgamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    return l\n",
    "end\n",
    "\n",
    "function dirmult_logpdf(X::AbstractMatrix, α)\n",
    "    l  = zero(promote_type(eltype(X), eltype(α)))\n",
    "    for j in 1:size(X, 2)\n",
    "        l += dirmult_logpdf(view(X, :, j), α)\n",
    "    end\n",
    "    return l\n",
    "end\n",
    "\n",
    "function dirmult_score(X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    ∇ = zeros(T, length(α))\n",
    "    return dirmult_score!(∇, X, α)\n",
    "end\n",
    "\n",
    "function dirmult_score!(∇::Vector, X::AbstractMatrix, α::Vector)\n",
    "    fill!(∇, zero(eltype(∇)))\n",
    "    for j in 1:size(X, 2)\n",
    "        dirmult_score!(∇, view(X, :, j), α)\n",
    "    end\n",
    "    return ∇\n",
    "end\n",
    "\n",
    "function dirmult_score!(\n",
    "        ∇::Vector, \n",
    "        x::AbstractVector, \n",
    "        α::Vector\n",
    "    )\n",
    "    \n",
    "    T = promote_type(eltype(x), eltype(α))\n",
    "    xs = zero(eltype(x))\n",
    "    αs = zero(eltype(α))\n",
    "    for i in eachindex(x)\n",
    "        xs += x[i]\n",
    "        αs += α[i]\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            ∇[i] += Inf\n",
    "        elseif α[i] > 0\n",
    "            ∇[i] += digamma(x[i] + α[i]) - digamma(α[i])\n",
    "        end\n",
    "    end\n",
    "    if αs > 0\n",
    "        c = digamma(xs + αs) - digamma(αs)\n",
    "        for i in eachindex(x)\n",
    "            ∇[i] -= c\n",
    "        end\n",
    "    end\n",
    "    return ∇\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo(X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    d = zeros(T, length(α))\n",
    "    return dirmult_obsinfo!(d, X, α)\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(d::Vector, X::AbstractMatrix, α::Vector)\n",
    "    T = promote_type(eltype(X), eltype(α))\n",
    "    c = zero(T)\n",
    "    fill!(d, zero(eltype(d)))\n",
    "    for j in 1:size(X, 2)\n",
    "        c += dirmult_obsinfo!(d, view(X, :, j), α)\n",
    "    end\n",
    "    return c, d\n",
    "end\n",
    "\n",
    "function dirmult_obsinfo!(d::Vector, x::AbstractVector, α::Vector)\n",
    "    T  = promote_type(eltype(x), eltype(α))\n",
    "    xs = zero(eltype(x))\n",
    "    αs = zero(eltype(α))\n",
    "    for i in eachindex(x)\n",
    "        xs += x[i]\n",
    "        αs += α[i]\n",
    "        if α[i] == 0 && x[i] > 0\n",
    "            d[i] += T(Inf)\n",
    "        elseif α[i] > 0\n",
    "            d[i] += T(trigamma(α[i]) - trigamma(x[i] + α[i]))\n",
    "        end\n",
    "    end\n",
    "    if αs == 0 && xs > 0\n",
    "        c = T(Inf)\n",
    "    elseif αs == 0 && xs == 0\n",
    "        c = zero(T)\n",
    "    elseif αs > 0\n",
    "        c = T(trigamma(αs) - trigamma(xs + αs))\n",
    "    end\n",
    "    return c\n",
    "end\n",
    "\n",
    "function dirmult_mom{T <: Real}(X::Matrix{T})\n",
    "    d, n   = size(X)\n",
    "    α      = zeros(Float64, d)\n",
    "    pi1    = zeros(Float64, d)\n",
    "    pi2    = zeros(Float64, d)\n",
    "    total  = zero(T)\n",
    "    for j in 1:n\n",
    "        colsum = zero(T)\n",
    "        for i in 1:d\n",
    "            α[i]   += X[i, j] # accumulate row sum\n",
    "            colsum += X[i, j]\n",
    "        end\n",
    "        total += colsum\n",
    "        if colsum > 0\n",
    "            for i in 1:d\n",
    "                pi      = X[i, j] / colsum\n",
    "                pi1[i] += pi   # accumulate pi\n",
    "                pi2[i] += pi^2 # accumulate pi^2\n",
    "            end  \n",
    "        end\n",
    "    end\n",
    "    # estiamte ρ and p\n",
    "    ρ = zero(Float64)\n",
    "    for i in 1:d\n",
    "        if pi1[i] > 0\n",
    "            ρ += pi2[i] / pi1[i]\n",
    "        end\n",
    "        α[i] /= total\n",
    "    end\n",
    "    αsum = max((d - ρ) / (ρ - 1), 1e-6)\n",
    "    for i in 1:d\n",
    "        α[i] *= αsum\n",
    "    end\n",
    "    return α\n",
    "end\n",
    "\n",
    "function dirmult_newton(\n",
    "        X::AbstractMatrix; \n",
    "        α0::Vector{Float64} = dirmult_mom(X), \n",
    "        maxiters::Int = 100, \n",
    "        tolfun::Float64 = 1e-6,\n",
    "        verbose::Bool = false\n",
    "    )\n",
    "    \n",
    "    T = promote_type(eltype(X), eltype(α0))\n",
    "    # only consider rows and columns with sum >0\n",
    "    rowind = find(sum(X, 2)) # rows with row sums > 0\n",
    "    colind = find(sum(X, 1)) # cols with col sums > 0\n",
    "    Xwork  = @view X[rowind, colind]\n",
    "    αwork  = α0[rowind]\n",
    "    \n",
    "    # pre-allocate intermediate variables\n",
    "    ∇            = similar(αwork)\n",
    "    obsinfo_dinv = similar(αwork)\n",
    "    newtondir    = similar(αwork)\n",
    "    αnew         = similar(αwork)\n",
    "    loglnew      = zero(T)\n",
    "    \n",
    "    # initial log-L\n",
    "    logliter = dirmult_logpdf(Xwork, αwork)\n",
    "    if verbose\n",
    "        println(\"iteration 0, logl = \", logliter)\n",
    "    end\n",
    "    \n",
    "    # Newton loop\n",
    "    niter = 0\n",
    "    for iter in 1:maxiters\n",
    "        # evaluate gradient (score)\n",
    "        dirmult_score!(∇, Xwork, αwork)\n",
    "        \n",
    "        # approximated observed information matrix\n",
    "        obsinfo_cinv, = dirmult_obsinfo!(obsinfo_dinv, Xwork, αwork)\n",
    "        obsinfo_cinv  = 1 / obsinfo_cinv\n",
    "        map!(x -> 1 / x, obsinfo_dinv)\n",
    "        # shrink c if necessary to make obs. inf. pos def\n",
    "        tmp = sum(obsinfo_dinv)\n",
    "        if obsinfo_cinv ≤ tmp\n",
    "            if verbose; println(\"shrink c\"); end\n",
    "            obsinfo_cinv = 1.05 * tmp\n",
    "        end\n",
    "        \n",
    "        # compute Newton's direction\n",
    "        newtondir .= obsinfo_dinv .* ∇\n",
    "        newtondir .+= (sum(newtondir) / (obsinfo_cinv - tmp)) .* obsinfo_dinv\n",
    "        \n",
    "        # make sure Newton iterate always land within boundary\n",
    "        stepsize = one(T)\n",
    "        for i in eachindex(αwork)\n",
    "            if newtondir[i] < 0\n",
    "                stepsize = min(- αwork[i] / newtondir[i] * 0.95, stepsize)\n",
    "            end\n",
    "        end\n",
    "        # line search loop\n",
    "        for lsiter in 1:10\n",
    "            αnew .= αwork .+ stepsize .* newtondir\n",
    "            loglnew = dirmult_logpdf(Xwork, αnew)\n",
    "            if loglnew > logliter\n",
    "                break\n",
    "            elseif lsiter == 10\n",
    "                println(\"line search failed\")\n",
    "            else\n",
    "                if verbose; println(\"step halving\"); end\n",
    "                stepsize /= 2\n",
    "            end\n",
    "        end\n",
    "        copy!(αwork, αnew)\n",
    "        loglold  = logliter\n",
    "        logliter = loglnew\n",
    "        \n",
    "        # print iterate log-L if requested\n",
    "        if verbose\n",
    "            println(\"iteration \", iter, \", logl = \", logliter)\n",
    "        end\n",
    "        \n",
    "        # check convergence criterion\n",
    "        if abs(logliter - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            niter = iter\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # compute logl, gradient, Hessian from final iterate\n",
    "    αfinal = zeros(eltype(α0), length(α0))\n",
    "    αfinal[rowind] = αwork\n",
    "    ∇final = zeros(eltype(α0), length(α0))\n",
    "    ∇final[rowind] = dirmult_score(Xwork, αwork)\n",
    "    obsinfo = zeros(eltype(α0), length(α0), length(α0))\n",
    "    obsinfo_c, obsinfo_d = dirmult_obsinfo!(obsinfo_dinv, Xwork, αwork)\n",
    "    obsinfo[rowind, rowind] = diagm(obsinfo_d) - obsinfo_c\n",
    "    \n",
    "    # output\n",
    "    return logliter, niter, αfinal, ∇final, obsinfo\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition dirmult_mm(AbstractArray{T<:Any, 2}) in module Main at In[25]:29 overwritten at In[41]:29.\n",
      "WARNING: Method definition #dirmult_mm(Array{Any, 1}, Main.#dirmult_mm, AbstractArray{T<:Any, 2}) in module Main overwritten.\n",
      "\u001b[1m\u001b[31mWARNING: replacing docs for 'dirmult_mm :: Tuple{AbstractArray{T,2}}' in module 'Main'.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dirmult_mm"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    dirmult_mm(X)\n",
    "\n",
    "Find the MLE of Dirichlet-multinomial distribution using MM algorithm.\n",
    "\n",
    "# Argument\n",
    "* `X`: an `d`-by-`n` matrix of counts; each column is one data point.\n",
    "\n",
    "# Optional argument  \n",
    "* `alpha0`: starting point. \n",
    "* `maxiters`: the maximum allowable Newton iterations (default 100). \n",
    "* `tolfun`: the tolerance for  relative change in objective values (default 1e-6). \n",
    "\n",
    "# Output\n",
    "# Output\n",
    "* `logl`: the log-likelihood at MLE.   \n",
    "* `niter`: the number of iterations performed.\n",
    "# `α`: the MLE.\n",
    "* `∇`: the gradient at MLE. \n",
    "* `obsinfo`: the observed information matrix at MLE. \n",
    "\"\"\"\n",
    "function dirmult_mm(\n",
    "        X::AbstractMatrix;  \n",
    "        α0::Vector = dirmult_mom(X.'),\n",
    "        maxiters::Int = 100, \n",
    "        tolfun = 1e-6,\n",
    "    )\n",
    "    #Assumes X is tall\n",
    "    n = size(X, 1)\n",
    "    nonZeros = find(sum(X,1) .!= 0.0)\n",
    "    Xnew = X[:, nonZeros]\n",
    "    αwork = α0[nonZeros]\n",
    "    Xt = Xnew.'\n",
    "    d = size(Xnew, 2)\n",
    "\n",
    "    #set intial values needed\n",
    "    sumα = sum(αwork)\n",
    "    sum1 = sum(Xnew, 1)\n",
    "    sum2 = sum(Xnew, 2)\n",
    "    αnew = zeros(size(αwork))\n",
    "    obsinfo_dinv = similar(αwork)\n",
    "    logl = 0.0\n",
    "    \n",
    "    maxij = zeros(d)\n",
    "    \n",
    "    s = zeros(d, Int(maximum(X)))\n",
    "\n",
    "    #Set sjk and rk as given in previous questions\n",
    "    for j in 1:d\n",
    "        maxij[j] = Int(maximum(Xt[j, :] .- 1))\n",
    "        for k in 0:Int(maxij[j])\n",
    "            for i in 1:n\n",
    "                if Xnew[i, j] > k\n",
    "                    s[j, k + 1] += 1.0\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "    end\n",
    "\n",
    "    maxi = Int(maximum(sum2 - 1))\n",
    "    \n",
    "    r = zeros(maxi + 1)\n",
    "    \n",
    "    for k in 0:maxi\n",
    "        for i in 1:n\n",
    "            if sum2[i] > k\n",
    "                r[k + 1] += 1.0\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    count = 0 \n",
    "    for iter in 1:maxiters\n",
    "        #apply update for each aj\n",
    "        for j in 1:d\n",
    "            dem = 0.0\n",
    "            for k in 0:maxi\n",
    "                dem += r[k + 1] / (sum(αwork) + k)\n",
    "            end\n",
    "            num = 0.0\n",
    "            for k in 0:Int(maxij[j])\n",
    "                num += s[j, k + 1] / (αwork[j] + k)\n",
    "            end\n",
    "            αnew[j] = num * αwork[j] ./ dem\n",
    "        end\n",
    "\n",
    "        # check convergence criterion        \n",
    "        logl = dirmult_logpdf(Xt, αnew)\n",
    "        loglold = dirmult_logpdf(Xt, αwork)\n",
    "        if abs(logl - loglold) < tolfun * (abs(loglold) + 1)\n",
    "            count = iter\n",
    "            break;\n",
    "        end\n",
    "        copy!(αwork, αnew)\n",
    "        count += 1\n",
    "    end\n",
    "\n",
    "    #generate output\n",
    "    αfin = zeros(size(X, 2))\n",
    "    αfin[nonZeros] = αwork\n",
    "    ∇final = zeros(eltype(αfin), length(αfin))\n",
    "    ∇final[nonZeros] = dirmult_score(Xnew', αwork)\n",
    "    obsinfo = zeros(eltype(α0), length(α0), length(α0))\n",
    "    obsinfo_c, obsinfo_d = dirmult_obsinfo!(obsinfo_dinv, Xt, αwork)\n",
    "    obsinfo[nonZeros, nonZeros] = diagm(obsinfo_d) - obsinfo_c \n",
    "    \n",
    "    return logl, count, αfin, ∇final, obsinfo\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  23.40 MiB\n",
       "  allocs estimate:  271731\n",
       "  --------------\n",
       "  minimum time:     1.022 s (0.00% GC)\n",
       "  median time:      1.031 s (0.00% GC)\n",
       "  mean time:        1.034 s (0.00% GC)\n",
       "  maximum time:     1.050 s (0.00% GC)\n",
       "  --------------\n",
       "  samples:          5\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "O = readdlm(\"optdigits.tra\", ',')';\n",
    "X = O[1:64,:]'\n",
    "\n",
    "@benchmark dirmult_mm(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slow, very sad :( but it is mainly because of the log likelihood tolerance check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6\n",
    "\n",
    "Re-do [HW4 Q9](http://hua-zhou.github.io/teaching/biostatm280-2017spring/hw/hw04.html#Q9) using your new `dirmult_mm` function. Compare the number of iterations and run time by MM algorithm to those by Newton's method. Comment on the efficiency of Newton's algorithm vs MM algorithm for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.02648292 seconds\n",
      "elapsed time: 0.037608137 seconds\n",
      "elapsed time: 0.033389861 seconds\n",
      "elapsed time: 0.040407248 seconds\n",
      "elapsed time: 0.032940633 seconds\n",
      "elapsed time: 0.044442414 seconds\n",
      "elapsed time: 0.030395971 seconds\n",
      "elapsed time: 0.033017006 seconds\n",
      "elapsed time: 0.036298687 seconds\n",
      "elapsed time: 0.026053755 seconds\n"
     ]
    }
   ],
   "source": [
    "using DataFrames, Distributions\n",
    "\n",
    "\n",
    "traindata = readcsv(\"optdigits.tra\", Int)\n",
    "\n",
    "ndigit       = zeros(Int, 10)\n",
    "logl_dirmult = zeros(10)\n",
    "iter_dirmult = zeros(Int, 10)\n",
    "time_dirmult = zeros(10)\n",
    "αhat_dirmult = zeros(64, 10)\n",
    "logl_multnom = zeros(10)\n",
    "lrtpval      = zeros(10)\n",
    "\n",
    "for d in 0:9\n",
    "    # retrieve data for digit d\n",
    "    Xd = traindata[traindata[:, end] .== d, 1:64].'\n",
    "    ndigit[d + 1] = size(Xd, 2)\n",
    "    # fit Dirichlet-multinomial\n",
    "    tic()\n",
    "    logl_dirmult[d + 1], iter_dirmult[d + 1], αhat, = dirmult_newton(Xd)\n",
    "    αhat_dirmult[:, d + 1] = αhat\n",
    "    time_dirmult[d + 1] = toc()\n",
    "    # fit multinomial\n",
    "    p = vec(sum(Xd, 2))\n",
    "    p = p / sum(p)\n",
    "    posind = p .> 0\n",
    "    for j in 1:size(Xd, 2)\n",
    "        logl_multnom[d + 1] += logpdf(Multinomial(Int(sum(Xd[posind, j])), \n",
    "                p[posind]), Xd[posind, j])\n",
    "    end\n",
    "    # compute LRT p-value\n",
    "    lrtpval[d + 1] = ccdf(Chisq(1), logl_dirmult[d + 1] - logl_multnom[d + 1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10×7 DataFrames.DataFrame\n",
      "│ Row │ digit │ n   │ logl_dm  │ logl_mn  │ iters │ LRT │ runtime   │\n",
      "├─────┼───────┼─────┼──────────┼──────────┼───────┼─────┼───────────┤\n",
      "│ 1   │ 0     │ 376 │ -37358.4 │ -39592.2 │ 5     │ 0.0 │ 0.0264829 │\n",
      "│ 2   │ 1     │ 389 │ -42179.2 │ -54039.2 │ 6     │ 0.0 │ 0.0376081 │\n",
      "│ 3   │ 2     │ 380 │ -39985.3 │ -49111.5 │ 6     │ 0.0 │ 0.0333899 │\n",
      "│ 4   │ 3     │ 389 │ -40519.5 │ -47089.1 │ 7     │ 0.0 │ 0.0404072 │\n",
      "│ 5   │ 4     │ 387 │ -43488.8 │ -57344.1 │ 5     │ 0.0 │ 0.0329406 │\n",
      "│ 6   │ 5     │ 376 │ -41191.3 │ -51713.0 │ 7     │ 0.0 │ 0.0444424 │\n",
      "│ 7   │ 6     │ 377 │ -37702.5 │ -42597.3 │ 6     │ 0.0 │ 0.030396  │\n",
      "│ 8   │ 7     │ 387 │ -40304.0 │ -49473.0 │ 6     │ 0.0 │ 0.033017  │\n",
      "│ 9   │ 8     │ 380 │ -43130.8 │ -49695.9 │ 6     │ 0.0 │ 0.0362987 │\n",
      "│ 10  │ 9     │ 382 │ -43709.7 │ -54577.8 │ 4     │ 0.0 │ 0.0260538 │"
     ]
    }
   ],
   "source": [
    "#copy of the output Dr. Zhou had for his Newton Method\n",
    "result = DataFrame(digit = 0:9, n = ndigit, \n",
    "    logl_dm = logl_dirmult, logl_mn = logl_multnom,\n",
    "    iters = iter_dirmult, LRT = lrtpval, runtime = time_dirmult)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 0.310849204 seconds\n",
      "elapsed time: 0.14953353 seconds\n",
      "elapsed time: 0.023714175 seconds\n",
      "elapsed time: 0.172134114 seconds\n",
      "elapsed time: 0.098797744 seconds\n",
      "elapsed time: 0.127212084 seconds\n",
      "elapsed time: 0.054326186 seconds\n",
      "elapsed time: 0.024536612 seconds\n",
      "elapsed time: 0.069113174 seconds\n",
      "elapsed time: 0.133868682 seconds\n"
     ]
    }
   ],
   "source": [
    "ndigit       = zeros(Int, 10)\n",
    "logl_dirmult = zeros(10)\n",
    "iter_dirmult = zeros(Int, 10)\n",
    "time_dirmult = zeros(10)\n",
    "αhat_dirmult = zeros(64, 10)\n",
    "logl_multnom = zeros(10)\n",
    "lrtpval      = zeros(10)\n",
    "\n",
    "for d in 0:9\n",
    "    # retrieve data for digit d\n",
    "    Xd = traindata[traindata[:, end] .== d, 1:64].'\n",
    "    ndigit[d + 1] = size(Xd, 2)\n",
    "    # fit Dirichlet-multinomial\n",
    "    tic()\n",
    "    logl_dirmult[d + 1], iter_dirmult[d + 1], αhat, = dirmult_mm(Xd.')\n",
    "    αhat_dirmult[:, d + 1] = αhat\n",
    "    time_dirmult[d + 1] = toc()\n",
    "    # fit multinomial\n",
    "    p = vec(sum(Xd, 2))\n",
    "    p = p / sum(p)\n",
    "    posind = p .> 0\n",
    "    for j in 1:size(Xd, 2)\n",
    "        logl_multnom[d + 1] += logpdf(Multinomial(Int(sum(Xd[posind, j])), \n",
    "                p[posind]), Xd[posind, j])\n",
    "    end\n",
    "    # compute LRT p-value\n",
    "    lrtpval[d + 1] = ccdf(Chisq(1), logl_dirmult[d + 1] - logl_multnom[d + 1])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10×7 DataFrames.DataFrame\n",
      "│ Row │ digit │ n   │ logl_dm  │ logl_mn  │ iters │ LRT │ runtime   │\n",
      "├─────┼───────┼─────┼──────────┼──────────┼───────┼─────┼───────────┤\n",
      "│ 1   │ 0     │ 376 │ -37361.9 │ -39592.2 │ 100   │ 0.0 │ 0.310849  │\n",
      "│ 2   │ 1     │ 389 │ -42179.5 │ -54039.2 │ 47    │ 0.0 │ 0.149534  │\n",
      "│ 3   │ 2     │ 380 │ -39985.3 │ -49111.5 │ 6     │ 0.0 │ 0.0237142 │\n",
      "│ 4   │ 3     │ 389 │ -40519.9 │ -47089.1 │ 48    │ 0.0 │ 0.172134  │\n",
      "│ 5   │ 4     │ 387 │ -43489.0 │ -57344.1 │ 26    │ 0.0 │ 0.0987977 │\n",
      "│ 6   │ 5     │ 376 │ -41191.6 │ -51713.0 │ 36    │ 0.0 │ 0.127212  │\n",
      "│ 7   │ 6     │ 377 │ -37703.0 │ -42597.3 │ 16    │ 0.0 │ 0.0543262 │\n",
      "│ 8   │ 7     │ 387 │ -40304.1 │ -49473.0 │ 6     │ 0.0 │ 0.0245366 │\n",
      "│ 9   │ 8     │ 380 │ -43131.3 │ -49695.9 │ 20    │ 0.0 │ 0.0691132 │\n",
      "│ 10  │ 9     │ 382 │ -43709.9 │ -54577.8 │ 39    │ 0.0 │ 0.133869  │"
     ]
    }
   ],
   "source": [
    "#My out for MM method\n",
    "result = DataFrame(digit = 0:9, n = ndigit, \n",
    "    logl_dm = logl_dirmult, logl_mn = logl_multnom,\n",
    "    iters = iter_dirmult, LRT = lrtpval, runtime = time_dirmult)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the objective Log densities between the two methods are comporable. However, the diagnositics look quite different. Newton's method achieves convergences in much fewer steps and is much less time than MM. This was suprising to me at first. However, through some tests I found that a majority of this time comes from the evaulation of the log densities for the tolerance check. In fact, if we forgo this step and just let the MM alorythm run for all interations, I saw a dramatic speed up. This makes sense because the update is much simpler. With this in mind, I would say that despite the run times listed above, MM can still be very efficient. The implementation is quite easy; there is no need to calculate the information matrix or the score functions, so this removes much user specified analytics when implementing. For MM, if we were to forgo the tolerance check, it would show to be much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7\n",
    "\n",
    "Finally let us re-consider the EM algorithm. The difficulty with the M step in EM algorithm can be remedied. Discuss how we can further minorize the $\\ln \\Gamma(|\\alpha|)$ term in the $Q$ function to produce a minorizing function with all $\\alpha_j$ separated. For this homework, you do **not** need to implement this EM-MM hybrid algorithm. Hint: $z \\mapsto \\ln \\Gamma(z)$ is a convex function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7) In order to further minorize $\\ln\\Gamma(|\\alpha|)$, we can use that it is convex. This yields that \n",
    "\n",
    "$$\\ln\\Gamma(|\\alpha|)\\ge \\Psi(|\\alpha^{(t)}|)(|\\alpha|-|\\alpha^{(t)}|) + \\ln\\Gamma(|\\alpha^{(t)}|)$$\n",
    "\n",
    "In corporating this into our previous $Q(\\alpha|\\alpha^{(t)})$, we can further minorize the likelihood functions resulting in all $\\alpha_j$ being separated and able to be independently updated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.1",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "65px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
